% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/algorithm.R
\name{algorithm_boss}
\alias{algorithm_boss}
\title{Algorithm BOSS}
\usage{
algorithm_boss(
  num_starts = 1,
  time_lag = 0,
  allow_internal_randomness = FALSE,
  use_bes = FALSE
)
}
\arguments{
\item{num_starts}{The number of times the algorithm should be re-run
from different random starting permutations. The model with the most
optimal BIC score will be selected. Random after the first. Defaults to 1.}

\item{time_lag}{This creates a time-series model automatically with a certain
number of lags. Defaults to zero.}

\item{allow_internal_randomness}{If true, the algorithm allow the algorithm to
use certain heuristic random steps. This can improve performance,
but may make the algorithm non-deterministic.}

\item{use_bes}{True if the final BES (Backward Equivalence Search) step is
used from the GES (Greedy Equivalence Search) algorithm.
This step is needed for correctness but for large models,
since usually nearly all edges are oriented in the CPDAG,
it is heurically not needed.}
}
\description{
Compose the flags of the BOSS algorithm
}
\details{
BOSS (Best Order Score Search) is an algorithm that, like GRaSP,
generalizes and extends the GSP (Greedy Sparsest Permutation) algorithm.
It has been tested to 1000 variables with an average degree of 20 and gives
near perfect precisions and recalls for N = 10,000
(with recall that drop to 0.9 for N = 1000).

The algorithms works by building DAGs given permutations in ways similar
to those described in Raskutti and Uhler and Solus et al. (see references below)

Knowledge of forbidden edges and required edges may be used with this algorithm.
Also, knowledge of tiers may be used. If tiered knowledge is supplied,
the algorithm will analyze the tiers in order, so that the time required
for the algorithm is linear in the number of tiers.

For more details, see: https://www.phil.cmu.edu/tetrad-javadocs/7.4.0/edu/cmu/tetrad/search/Boss.html
and https://cmu-phil.github.io/tetrad/manual/#boss
}
\references{
Dimitris Margaritis and Sebastian Thrun. Bayesian network induction via local neighborhoods. Advances in neural information processing systems, 12, 1999.

Raskutti, G., & Uhler, C. (2018). Learning directed acyclic graph models based on sparsest permutations. Stat, 7(1), e183.

Solus, L., Wang, Y., Matejovicova, L., & Uhler, C. (2017). Consistency guarantees for permutation-based causal inference algorithms. arXiv preprint arXiv:1702.03530.

Lam, W. Y., Andrews, B., & Ramsey, J. (2022, August). Greedy relaxations of the sparsest permutation algorithm. In Uncertainty in Artificial Intelligence (pp. 1052-1062). PMLR.
}
