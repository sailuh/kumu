---
title: "Issue Causal Analysis"
output: 
  html_document:
    toc: true
    number_sections: true
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Issue Causal Analysis}
  %\VignetteEncoding{UTF-8}
---

# Introduction

```{r warning = FALSE, message = FALSE}
rm(list = ls())
seed <- 1
set.seed(seed)

require(kumu)
require(stringi)
require(data.table)
require(knitr)
require(lubridate)
require(visNetwork)
require(igraph)
require(colorBlindness)
```

This notebook performs the necessary data transformations to the final table generated by [issue_social_smell_showcase.Rmd](https://github.com/sailuh/kaiaulu/blob/master/vignettes/issue_social_smell_showcase.Rmd) Notebook in order to perform Causal Analysis using Kumu and Tetrad's Causal Command.

```{r}
#dt <- fread("~/causal_tse/causal_modelling/1_openssl_social_smells_timeline.csv")
dt <- fread("~/Downloads/final_ist_cve_smell_interval_dt.csv")
```


# Feature Engineering

## Formatting Data Types

In order to be loaded in Tetrad, some variables must be transformed from String to Integer due to data type limitations. 

### CVE Data Type

We concatenate the last two digits of the year with the last four digits of the cve_id and convert into an integer. (E.g. 2006 and CVE ID XXX4339 becomes 06339).

```{r}
last_two_digits_year <- stringi::stri_sub(dt$cve_id,from=7,to = 8)
last_four_digits_cve <- stringi::stri_sub(dt$cve_id,from=10,to = 14)
dt$cve_id <- as.integer(stringi::stri_c(last_two_digits_year,last_four_digits_cve))
```

Second, commit interval is transformed into `activity_0` and `activity_2` if the commit hash is missing or available respectively:

```{r}
dt$activity_0 <- ifelse(dt$commit_interval == "",1,0)
dt$activity_2 <- ifelse(dt$commit_interval != "",1,0)
```


### Convert "start" to Unix Timestamp

To use start in causal analysis, we convert it to a unix timestamp. 

```{r}
dt$start <- as.numeric(dt$start)
```

## Feature Renaming

A number of feature names are also shortened, so their visual representation do not take too much screen space:

```{r}
setnames(x=dt,
         old = c("start_datetime",
                 "missing_links",
                 "radio_silence",
                 "code_only_devs",
                 "code_files",
                 "ml_only_devs",
                 "ml_threads",
                 "n_commits",
                 "churn"),
         new = c("start",
                 "mis_link",
                 "silence",
                 "code_dev",
                 "file",
                 "mail_dev",
                 "thread",
                 "commit",
                 "churn"))

dt <- dt[,.(cve_id,
            activity_0,
            activity_2,
            start,
            org_silo,
            mis_link,
            silence,
            code_dev,
            file,
            mail_dev,
            thread,
            commit,
            churn
            )]
```



## Missing Data Handling

We decided to remove rows from the dataset for which the mailing list data source is missing (i.e. 2000-2001).

```{r}
dt$start <- lubridate::ymd_hms(dt$start)
dt <- dt[(year(start) < 2000) | (year(start) > 2001)]
```

With respect to data missing due to inactivity during a given time period, any measures of features (counts) related to commits should all be 0.

```{r}
setnafill(dt, cols = colnames(dt), fill = 0)
```


## 1-Time Lag Features

```{r}
add_time_lag <- function(cve_table){
  table <- cve_table
  
  if(nrow(table) < 2){
       lag_table <- cbind(table,
                          table[,.(org_silo2 = NA,
                                   mis_link2 = NA,
                                   silence2 = NA,
                                   code_dev2 = NA,
                                   file2 = NA,
                                   mail_dev2 = NA,
                                   thread2 = NA,
                                   commit2 = NA,
                                   churn2 = NA)])
  }else{
     lag_table <- cbind(table[1:(nrow(table)-1)],
                     table[2:nrow(table),
                           .(org_silo2 = org_silo,
                             mis_link2 = mis_link,
                             silence2 = silence,
                             code_dev2 = code_dev,
                             file2 = file,
                             mail_dev2 = mail_dev,
                             thread2 = thread,
                             commit2 = commit,
                             churn2 = churn)])
  }
  return(lag_table)
}
```

```{r}
lag_dt <- dt[order(cve_id,start)][, add_time_lag(.SD),
             by = c("cve_id")]
```

## Remove Short CVEs 

We deleted CVEs (their associated rows) with 7 or fewer time periods.

```{r}
short_cves <- lag_dt[,.(n_rows=.N),by="cve_id"][order(n_rows)][n_rows <= 7]
short_cves
```
```{r}
short_cve_ids <- short_cves$cve_id
lag_dt <- lag_dt[!(cve_id %in% short_cve_ids)]
```

## Addressing Determinism and High Intercorrelation Among Features

```{r}
cor_table <- lag_dt[,.(org_silo,
                       mis_link,
                       silence,
                       code_dev,
                       file,
                       mail_dev,
                       thread,
                       commit,
                       churn,
                       org_silo2,
                       mis_link2,
                       silence2,
                       code_dev2,
                       file2,
                       mail_dev2,
                       thread2,
                       commit2,
                       churn2)]
cor(cor_table)
```

Due to high correlation, we perform 6 feature deletions (activity_0, activity_2, org_silo, org_silo2):

```{r}
lag_dt <- lag_dt[,.(cve_id,
                    start,
                    mis_link,
                    silence,
                    code_dev,
                    file,
                    mail_dev,
                    thread,
                    commit,
                    churn,
                    mis_link2,
                    silence2,
                    code_dev2,
                    file2,
                    mail_dev2,
                    thread2,
                    commit2,
                    churn2)]
```

## Binarized CVE Indicators

To represent the CVE Ids, we utilize indicator features. For every CVE ID, a new column is added to the table which can take values 0 or 1. The value is 1 if the row is associated to that CVE ID, or 0 otherwise.

```{r}
# Extract only the cve_id column, assign that they should have 1 value 
# when dcasted, and an id column for the formula for dcast. 

binarize_cve_id <- lag_dt[,.(id = c(1:nrow(lag_dt)),
                             cve_id= stringi::stri_c("b_",cve_id),
                             binary_value = 1)]
binarize_cve_id <- dcast(binarize_cve_id,id ~ cve_id,
                         value.var = "binary_value",
                         fill=0)
head(cbind(cve_id=lag_dt$cve_id,binarize_cve_id))
```

We can then remove the `cve_id` column, as the binary features represent the same information, and add the remaining columns to the analysis table:

```{r}
# Remove cve_id
lag_dt <- lag_dt[,.(start,
                    mis_link,
                    silence,
                    code_dev,
                    file,
                    mail_dev,
                    thread,
                    commit,
                    churn,
                    mis_link2,
                    silence2,
                    code_dev2,
                    file2,
                    mail_dev2,
                    thread2,
                    commit2,
                    churn2)]

# Add all binary columns except cve_id from the new table
binarized_lag_dt <- cbind(lag_dt,binarize_cve_id[,(2:ncol(binarize_cve_id)),with=FALSE])
```

## Add Null Features

An example of the randomization only showing the silence and nv-silence is shown below. In practice, for every column in `lag_dt` up to this point, we generated a replica column prefixed by `nv-`, including the binary features (which are then prefixed as `nv-b_`), but the replica columns have their values shuffled across the rows, hence the null (random) naming to them.

```{r}
nv_lag_dt <- binarized_lag_dt
colnames(nv_lag_dt) <- stringi::stri_c("nv-",colnames(binarized_lag_dt))
nv_lag_dt <- apply(nv_lag_dt,2,sample)
nv_lag_dt <- cbind(binarized_lag_dt,nv_lag_dt)

head(nv_lag_dt[,.(silence,`nv-silence`)])
```

## Keep only 5 null indicator features

Introducing a null feature for all variables and features leads to too many features being introduced for causal search, causing heap memory errors in Tetrad. We preserve only a few of the nv binary indicator variables, as they lead to variable explosion and their pattern is easy to randomize. Position 138 includes all variables as null variables, plus five binary indicators as null variables. We consider this loss of null binary indicator features reasonable, as the randomization of a few blocks of values 1 or 0 will generally be equivalent. This in turn, allow us to perform more causal search runs, which we deem a fair trade-off. 

```{r}
nv_lag_dt <- nv_lag_dt[,1:138]
```


# FGES Null Variable Search

We save the dataset after feature engineering locally, so it can be used by Causal Command via Kumu. 

```{r}
nv_lag_dt_path <- "/tmp/null_variable_dt.csv" 
fwrite(nv_lag_dt,nv_lag_dt_path)
```

We then set the output file configuration. 

```{r}
dt_path <- nv_lag_dt_path
output_folder_path <- "~/projects/kumu_data/analysis/openssl/null_search"
filename <- "boss_bootstrap_null_search_1000_runs_nv_binary_indicators"
filepath <- stringi::stri_c(file.path(output_folder_path,filename),"_graph.json")
```

Finally, we perform causal search over our null dataset. In this Notebook we include both FGES and BOSS. To execute one or the other, modify `eval` to TRUE on either code block. This may take some time to execute. After the data is generated, the code block evaluation can be again set to FALSE, as the remaining analysis can be performed using the output file of either algorithms. 

The output `.json` file of Causal Search can also be loaded directly on Tetrad GUI for inspection.

This is the FGES search. Note in our local machine, we could not use the number of runs up to 1000, due to java memory heap errors.

```{r FGES Null Variable Causal Search, echo = FALSE, message = FALSE, eval = FALSE}

data_flags <- data_io(dataset_path = dt_path,
                         data_type = "continuous",
                         column_delimiter = "comma",
                         output_folder_path = output_folder_path,
                         filename = filename,
                         is_json_output = TRUE)

algorithm_flags <- algorithm_fges(max_degree = 1000,
                                  time_lag = 0,
                                  faithfulness_assumed = TRUE,
                                  meek_verbose = FALSE,
                                  parallelized = FALSE,
                                  symmetric_first_step = TRUE,
                                  verbose = TRUE)

score_flags <- score_sem_bic(penalty_discount = 2, 
                             sem_bic_rule = 1, 
                             sem_bic_structure_prior = 0, 
                             precompute_covariances = TRUE)

bootstrapping_flags <- bootstrapping(number_resampling=500,
                                     percent_resample_size = 90,
                                     seed = 32, 
                                     add_original_dataset = TRUE,
                                     resampling_with_replacement = TRUE,
                                     resampling_ensemble = 1,
                                     save_bootstrap_graphs = FALSE)

tetrad_path <- "~/projects/kumu/causal-cmd-1.11.1-jar-with-dependencies.jar"
tetrad(tetrad_cmd_path = tetrad_path,
       data_flags = data_flags, 
       algorithm_flags = algorithm_flags,
       score_flags = score_flags,
       bootstrapping_flags = bootstrapping_flags)
```

This is the BOSS Search. We found BOSS scaled better, allowing us to increase the number of bootstraps up to 1000. Observe also the knowledge box is not specified at this point: We do not impose any restrictions when observing the formation of edges at random. Our final conclusions are derived from BOSS. Note also that we bootstrap on BOSS with 100% of the dataset, while in FGES we do so with 90% of the dataset. We use 100% in BOSS, because the algorithm already has random initialization. 

```{r BOSS Null Variable Causal Search, echo = FALSE, message = FALSE, eval = FALSE}

data_flags <- data_io(dataset_path = dt_path,
                         data_type = "continuous",
                         column_delimiter = "comma",
                         output_folder_path = output_folder_path,
                         filename = filename,
                         is_json_output = TRUE)

algorithm_flags <- algorithm_boss(num_starts = 1,
                           num_threads = 15,
                           time_lag = 0,
                           use_bes = FALSE,
                           use_data_order = TRUE,
                           verbose = FALSE)

score_flags <- score_sem_bic(penalty_discount = 2, 
                             sem_bic_rule = 1, 
                             sem_bic_structure_prior = 0, 
                             precompute_covariances = TRUE)

bootstrapping_flags <- bootstrapping(number_resampling=1000,
                                     percent_resample_size = 100,
                                     seed = 32, 
                                     add_original_dataset = TRUE,
                                     resampling_with_replacement = TRUE,
                                     resampling_ensemble = 1,
                                     save_bootstrap_graphs = FALSE)

tetrad_path <- "~/projects/kumu/causal-cmd-1.11.1-jar-with-dependencies.jar"
tetrad(tetrad_cmd_path = tetrad_path,
       data_flags = data_flags, 
   #    knowledge_flags = knowledge_flags,
       algorithm_flags = algorithm_flags,
       score_flags = score_flags,
       bootstrapping_flags = bootstrapping_flags)
```

# Deriving the 1 PNEF Threshold

In our causal search above, we introduced null features over multiple bootstrap runs to observe how often our causal search form random edges (i.e. between our features and null features). We will use this information to derive a threshold, 1PNEF, we can use in our final causal search.


## Graph Examination

We now have our causal bootstrap graph as a .json file, which is output by Tetrad. Let's parse it into a tabular format to provide further intuition on how the 1 PNEF threshold is being determined. 

The nodes contain all our variables and null features In the off_chance a feature does not have any edge to it, this table allow us to still show it on the graph, as it would not appear on the "edge list" table.

```{r}
graph <- parse_graph(filepath)
head(graph[["nodes"]])
```

Next is the edgeset table output by Tetrad. This table contains all the edges. Because we are performing multiple executions, each with a sample of the full dataset (as we are using a "bootstrap" approach), the probabilities represented here are the "ensemble" of all edges formed on each execution. In this Notebook, the preserved ensemble was used.

```{r}
head(graph[["edgeset"]])
```

Lastly, we can examine the counts of each type of edge formed on each subgraph via the edge_type_probabilities table. Since the edgeset table probability already sums the probabilities from this table for every node pair, this information is presented here only for qualitative inspection, but it is not currently used in the subsequent steps.


```{r}
head(graph[["edge_type_probabilities"]])
```


## Deriving 1 PNEF

As noted, our interest is to derive a threshold for the final causal search, using the information of this bootstrapped null feature causal search between the actual variables, and the random features. By this randome dge definition, our first step is to subset the `edgeset`table  to contain only the edge pairs that include null variables. A sample is shown below of the table where at least one of the two nodes is nv:

```{r}
nv_edges <- data.table::copy(graph[["edgeset"]])
is_node1_nv <- stringi::stri_detect_regex(nv_edges$node1_name,pattern = "nv-")
is_node2_nv <- stringi::stri_detect_regex(nv_edges$node2_name,pattern = "nv-")
nv_edges <- nv_edges[is_node1_nv | is_node2_nv]
head(nv_edges)
```
Next, we can derive a no_edge probability by subtracting 1 from the `probability` value. 

```{r}
nv_edges$no_edge <- 1 - nv_edges$probability 
```

Our goal then is to identify the first percentile value of the no edge probability, i.e. the 1st percentile NoEdge Frequency value (1PNEF):

```{r}
pnef_1 <- quantile(nv_edges$no_edge,probs=0.01)
pnef_1
```

What this threshold tell us is that, if executed 1000 runs, then the first percentile of all random edges formed had approximately 65% no formation of causal link. Another way to state this is that given entirely random variables, causal links were formed between them up to 35% of the time. In our final search, we then only keep causal links that, over 1000 runs, formed **more** than 35% of the time, under the assumption any causal link established less than that may be due to random chance. 

With the threshold defined, we can now proceed to the final causal search, which does not include null features. In this non null feature causal search, we also specify domain knowledge. 

# Non-Null Causal Search

## Domain Knowledge Causal Search without Null Variables

Domain knowledge is used to prohibit causal links to form among features. Here, we only defined temporal causal link restrictions. I.e. it does not make sense for features at 1-time-lag (future) to cause features on the present time.

```{r}
binarized_lag_dt_path <- "/tmp/binarized_variable_dt.csv" 
fwrite(binarized_lag_dt,binarized_lag_dt_path)
```

```{r}
#knowledge_file_path <- "~/Downloads/knowledge_2.txt"
knowledge_file_path <- "~/projects/kumu_data/analysis/openssl/knowledge_box.txt"
knowledge_flags <- knowledge_file_path(knowledge_file_path)
```

## Causal Search

As before, we specify the graph output file. 

```{r}
dt_path <- binarized_lag_dt_path
output_folder_path <- "~/projects/kumu_data/analysis/openssl/domain_binarized_search"
filename <- "boss_bootstrap_binarized_search_1000_runs_binary_indicators"
filepath <- stringi::stri_c(file.path(output_folder_path,filename),"_graph.json")
```

We also have the choice of using FGES or BOSS here. In our final analysis, we used BOSS.

FGES Causal Search:

```{r FGES Causal Search without Null Variables, echo = FALSE, message= FALSE, eval = FALSE}

data_flags <- data_io(dataset_path = dt_path,
                         data_type = "continuous",
                         column_delimiter = "comma",
                         output_folder_path = output_folder_path,
                         filename = filename,
                         is_json_output = TRUE)

algorithm_flags <- algorithm_fges(max_degree = 1000,
                                  time_lag = 0,
                                  faithfulness_assumed = TRUE,
                                  meek_verbose = FALSE,
                                  parallelized = FALSE,
                                  symmetric_first_step = TRUE,
                                  verbose = TRUE)

score_flags <- score_sem_bic(penalty_discount = 2, 
                             sem_bic_rule = 1, 
                             sem_bic_structure_prior = 0, 
                             precompute_covariances = TRUE)

bootstrapping_flags <- bootstrapping(number_resampling=1000,
                                     percent_resample_size = 90,
                                     seed = 32, 
                                     add_original_dataset = TRUE,
                                     resampling_with_replacement = TRUE,
                                     resampling_ensemble = 1,
                                     save_bootstrap_graphs = FALSE)

tetrad_path <- "~/projects/kumu/causal-cmd-1.11.1-jar-with-dependencies.jar"
tetrad(tetrad_cmd_path = tetrad_path,
       data_flags = data_flags, 
       knowledge_flags = knowledge_flags,       
       algorithm_flags = algorithm_flags,
       score_flags = score_flags,
       bootstrapping_flags = bootstrapping_flags)
```

BOSS Causal Search:

```{r BOSS Causal Search without Null Variables, echo = FALSE, message = FALSE, eval = FALSE}

data_flags <- data_io(dataset_path = dt_path,
                         data_type = "continuous",
                         column_delimiter = "comma",
                         output_folder_path = output_folder_path,
                         filename = filename,
                         is_json_output = TRUE)

algorithm_flags <- algorithm_boss(num_starts = 1,
                           num_threads = 15,
                           time_lag = 0,
                           use_bes = FALSE,
                           use_data_order = TRUE,
                           verbose = FALSE)

score_flags <- score_sem_bic(penalty_discount = 2, 
                             sem_bic_rule = 1, 
                             sem_bic_structure_prior = 0, 
                             precompute_covariances = TRUE)

bootstrapping_flags <- bootstrapping(number_resampling=1000,
                                     percent_resample_size = 100,
                                     seed = 32, 
                                     add_original_dataset = TRUE,
                                     resampling_with_replacement = TRUE,
                                     resampling_ensemble = 1,
                                     save_bootstrap_graphs = FALSE)

tetrad_path <- "~/projects/kumu/causal-cmd-1.11.1-jar-with-dependencies.jar"
tetrad(tetrad_cmd_path = tetrad_path,
       data_flags = data_flags, 
       knowledge_flags = knowledge_flags,
       algorithm_flags = algorithm_flags,
       score_flags = score_flags,
       bootstrapping_flags = bootstrapping_flags)
```

# Applying 1PNEF Threshold

We load the final causal search, and then apply the 1PNEF threshold derived from the prior causal search here. A sample of the causal graph nodes and edges is shown below:

```{r}
graph <- parse_graph(filepath)
head(graph[["nodes"]])
```

```{r}
head(graph[["edgeset"]])
```

```{r}
head(graph[["edge_type_probabilities"]])
```
## Applying 1PNEF Threshold

Edges which may have been formed at random are filtered here:

```{r}
edges <- graph[["edgeset"]]
edges$no_edge <- 1 - edges$probability 
edges_1pnef <- edges[no_edge <= pnef_1]
edges_1pnef
```

# Results 

With the final causal graph trimmed, we can now inspect it to draw conclusions from it. Causal graphs may form cycles, and also have undirected edges. We define a function to check for cycles here and use it below for inspection.

```{r}
# Reference: https://stackoverflow.com/a/55094319/1260232
## More efficient version
find_cycles = function(g) {
    Cycles = NULL
    for(v1 in V(g)) {
        if(degree(g, v1, mode="in") == 0) { next }
        GoodNeighbors = neighbors(g, v1, mode="out")
        GoodNeighbors = GoodNeighbors[GoodNeighbors > v1]
        for(v2 in GoodNeighbors) {
            TempCyc = lapply(all_simple_paths(g, v2,v1, mode="out"), function(p) c(v1,p))
            TempCyc = TempCyc[which(sapply(TempCyc, length) > 3)]
          TempCyc = TempCyc[sapply(TempCyc, min) == sapply(TempCyc, `[`, 1)]
          Cycles  = c(Cycles, TempCyc)
        }
    }
    Cycles
}

```

## Full Causal Graph 1-PNEF Trimmed 

First, we can inspect the full causal graph.

```{r}
nodes <- data.table::copy(graph[["nodes"]])
colnames(nodes) <- "node"

edges <- copy(edges_1pnef)

edges$color <- "black"
edges[endpoint1 == "TAIL" & endpoint2 == "TAIL"]$color <- "red"
edges <- edges[,.(from=node1_name,to=node2_name,color=color,weight=probability,label=probability)]
```


```{r}

nodes_n <- copy(nodes)
nodes_n$color <- colorBlindness::Blue2DarkRed12Steps[3]
nodes_n[node %in% c("silence","silence2")]$color <- Blue2DarkRed12Steps[5]
nodes_n[node %in% c("mis_link","mis_link")]$color <- Blue2DarkRed12Steps[3]
nodes_n[node %in% c("code_dev","code_dev2")]$color <- Blue2DarkRed12Steps[7]
nodes_n[node %in% c("churn","churn2")]$color <- Blue2DarkRed12Steps[8]
nodes_n[node %in% c("commit","commit2")]$color <- Blue2DarkRed12Steps[9]
nodes_n[stringi::stri_detect(nodes_n$node, regex = "b_")]$color <- Blue2DarkRed12Steps[1] 

g <- igraph::graph_from_data_frame(d=edges, 
                      directed = TRUE, 
                      vertices = nodes_n)

g_viz <- visIgraph(g,
          randomSeed = 1)#,
          #layout = "layout_with_dh")
#vis_graph <- toVisNetworkData(graph)
#visNetwork(nodes = vis_graph$nodes, edges = vis_graph$edges,randomSeed = 1,
#           height = "600px", width = "100%") %>% 
g_viz %>% visOptions(highlightNearest = TRUE) %>% visInteraction(navigationButtons = TRUE)#  %>% 
  #visHierarchicalLayout()
  #visInteraction(navigationButtons = TRUE,keyboard = TRUE, tooltipDelay = 0 ) 

#visSave(g_viz,"~/Downloads/openssl_causal_graph_with_cycles.html")
```


## Sub-Graphs of Effort Variables and Parents

```{r}
nodes_of_interest <- c("silence","silence2",
                       "mis_link","mis_link2",
                       "code_dev","code_dev2",
                       "churn","churn2",
                       "commit","commit2")
edges_n<- copy(edges[from %in% nodes_of_interest & to %in% nodes_of_interest])
nodes_n <- copy(nodes[node %in% unique(c(edges_n$from,edges_n$to))])
nodes_n$color <- colorBlindness::Blue2DarkRed12Steps[3]
nodes_n[node %in% c("silence","silence2")]$color <- Blue2DarkRed12Steps[5]
nodes_n[node %in% c("mis_link","mis_link")]$color <- Blue2DarkRed12Steps[3]
nodes_n[node %in% c("code_dev","code_dev2")]$color <- Blue2DarkRed12Steps[7]
nodes_n[node %in% c("churn","churn2")]$color <- Blue2DarkRed12Steps[8]
nodes_n[node %in% c("commit","commit2")]$color <- Blue2DarkRed12Steps[9]

g_viz <- visIgraph(g,
          randomSeed = 1)#,
          #layout = "layout_with_dh")
#vis_graph <- toVisNetworkData(graph)
#visNetwork(nodes = vis_graph$nodes, edges = vis_graph$edges,randomSeed = 1,
#           height = "600px", width = "100%") %>% 
g_viz %>% visOptions(highlightNearest = TRUE) %>% visInteraction(navigationButtons = TRUE)#  %>% 
  #visHierarchicalLayout()
  #visInteraction(navigationButtons = TRUE,keyboard = TRUE, tooltipDelay = 0 ) 

#visSave(g_viz,"~/Downloads/openssl_causal_graph_with_cycles.html")
```

```{r}
find_cycles(g)
```
